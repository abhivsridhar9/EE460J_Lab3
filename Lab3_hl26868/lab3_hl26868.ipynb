{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Q2\n",
    "# http://proceedings.mlr.press/v70/\n",
    "\n",
    "# Python3 program for a word frequency\n",
    "# counter after crawling/scraping a web-page\n",
    "import requests\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import operator\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "import math\n",
    " \n",
    "'''Function defining the web-crawler/core\n",
    "spider, which will fetch information from\n",
    "a given website, and push the contents to\n",
    "the second  function clean_wordlist()'''\n",
    " \n",
    " \n",
    "def start(url):\n",
    "\n",
    "    root = pathlib.Path().resolve()\n",
    "    # #If there is no such folder, the script will create one automatically\n",
    "    # folder_location = 'pdfs'\n",
    "    # if not os.path.exists(folder_location):os.mkdir(folder_location)\n",
    "\n",
    "    # # clear folder\n",
    "    # os.system('rm pdfs/*')\n",
    "\n",
    "    # response = requests.get(url)\n",
    "    # soup= BeautifulSoup(response.text, \"html.parser\")     \n",
    "    # for link in soup.select(\"a[href$='.pdf']\"):\n",
    "    #     #Name the pdf files using the last portion of each link which are unique in this case\n",
    "    #     filename = os.path.join(folder_location,link['href'].split('/')[-1])\n",
    "    #     # print('{0} {1}\\n'.format(link['href'].split('/')[-1], filename))\n",
    "    #     with open(filename, 'wb') as f:\n",
    "    #         f.write(requests.get(urljoin(url,link['href'])).content)\n",
    "    #         os.system(\"pdf2txt.py \" + filename + \" > \" + (os.path.splitext(filename)[0] + '.txt'))\n",
    "    #     os.remove(filename)\n",
    " \n",
    "    wordlist = []\n",
    "    for filename in os.listdir('pdfs'):\n",
    "        path = os.path.join(root, ('pdfs/'+filename))\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                wordlist.extend(line.split())\n",
    "\n",
    "    print('read all files and created word list\\n')\n",
    "\n",
    "    # # empty list to store the contents of\n",
    "    # # the website fetched from our web-crawler\n",
    "\n",
    "    # source_code = requests.get(url).text\n",
    " \n",
    "    # # BeautifulSoup object which will\n",
    "    # # ping the requested url for data\n",
    "    # soup = BeautifulSoup(source_code, 'html.parser')\n",
    " \n",
    "    # # Text in given web-page is stored under\n",
    "    # # the <div> tags with class <entry-content>\n",
    "    # for each_text in soup.findAll('div', {'class': 'entry-content'}):\n",
    "    #     content = each_text.text\n",
    " \n",
    "    #     # use split() to break the sentence into\n",
    "    #     # words and convert them into lowercase\n",
    "    #     words = content.lower().split()\n",
    " \n",
    "    #     for each_word in words:\n",
    "    #         wordlist.append(each_word)\n",
    "    clean_wordlist(wordlist)\n",
    " \n",
    "# Function removes any unwanted symbols\n",
    "\n",
    " \n",
    "def clean_wordlist(wordlist):\n",
    " \n",
    "    clean_list = []\n",
    "    symbols = \"!@#$%^&*()_-+={[}]|\\;:\\\"<>?/., \"\n",
    "    for word in wordlist:\n",
    "        for i in range(len(symbols)):\n",
    "            word = word.replace(symbols[i], '')\n",
    " \n",
    "        if len(word) > 0:\n",
    "            clean_list.append(word)\n",
    "    create_dictionary(clean_list)\n",
    " \n",
    "# Creates a dictionary containing each word's\n",
    "# count and top_20 ocuuring words\n",
    " \n",
    " \n",
    "def create_dictionary(clean_list):\n",
    "    word_count = {}\n",
    " \n",
    "    for word in clean_list:\n",
    "        if word in word_count:\n",
    "            word_count[word] += 1\n",
    "        else:\n",
    "            word_count[word] = 1\n",
    " \n",
    "    ''' To get the count of each word in\n",
    "        the crawled page -->\n",
    " \n",
    "    # operator.itemgetter() takes one\n",
    "    # parameter either 1(denotes keys)\n",
    "    # or 0 (denotes corresponding values)\n",
    " \n",
    "    for key, value in sorted(word_count.items(),\n",
    "                    key = operator.itemgetter(1)):\n",
    "        print (\"% s : % s \" % (key, value))\n",
    " \n",
    "    <-- '''\n",
    "    \n",
    "    c = Counter(word_count)\n",
    " \n",
    "    # returns the most occurring elements\n",
    "    top = c.most_common(10)\n",
    "    print(top)\n",
    "\n",
    "    df = pd.DataFrame(top, columns=['Word', 'Count'])\n",
    "    df.plot.bar(x='Word', y='Count') \n",
    "\n",
    "    entropy = 0\n",
    "    # part 2\n",
    "    total_wc = sum(word_count.values())\n",
    "    for word, count in word_count.items():\n",
    "        count = count * 1.0 / total_wc\n",
    "    for word, prob in word_count.items():\n",
    "        p = int(prob)\n",
    "        entropy += p * math.log2(p)\n",
    "    entropy = -1 * entropy\n",
    "\n",
    "    print('Z entropy: {}'.format(entropy))\n",
    " \n",
    "# Driver code\n",
    "if __name__ == '__main__':\n",
    "    url = \"http://proceedings.mlr.press/v70/\"\n",
    "    # starts crawling and prints output\n",
    "    start(url)\n",
    "    print('program ran\\n') "
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pdfs'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-72efee4c9219>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    139\u001B[0m     \u001B[0murl\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"http://proceedings.mlr.press/v70/\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    140\u001B[0m     \u001B[0;31m# starts crawling and prints output\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 141\u001B[0;31m     \u001B[0mstart\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0murl\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    142\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'program ran\\n'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    143\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-1-72efee4c9219>\u001B[0m in \u001B[0;36mstart\u001B[0;34m(url)\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m     \u001B[0mwordlist\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 45\u001B[0;31m     \u001B[0;32mfor\u001B[0m \u001B[0mfilename\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlistdir\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'pdfs'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     46\u001B[0m         \u001B[0mpath\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mroot\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34m'pdfs/'\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'r'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'pdfs'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Q2 part 2\n",
    "# Estimate entropy of Z - randomly selected word from one ICML paper\n",
    "# Entropy formula:\n",
    "# H = - Sum (p * log(p))\n",
    "\n",
    "# find pmf of Z\n",
    "# words = {'word': wordcount}"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}